---
title: "EfficientNet Training with Lightning"
format: 
    html:
        embed-resources: true
---


```{python}
import torch
import torch.nn as nn
import torchvision
import torchmetrics
import kornia.augmentation as K
import lightning as L
import torch.nn.functional as F
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import numpy as np
from pathlib import Path

from IPython.display import display
from kornia import image_to_tensor, tensor_to_image
from kornia.augmentation import ColorJitter, RandomChannelShuffle, RandomHorizontalFlip, RandomThinPlateSpline
from pytorch_lightning.loggers import CSVLogger
from torch import Tensor

# Reduced the batch size to 32 from 64 for EfficientNetB1, 16 B2
BATCH_SIZE = 64
TORCH_SEED = 12
TORCH_CUDA_SEED = 12
```

```{python}
class DataAugmentation(nn.Module):
    """Module to perform data augmentation using Kornia on torch tensors."""

    def __init__(self, apply_color_jitter: bool = False) -> None:
        super().__init__()
        self._apply_color_jitter = apply_color_jitter

        self.transforms = nn.Sequential(
            K.RandomHorizontalFlip(p=0.5),
            K.RandomPerspective(distortion_scale=0.5, p = 0.3),
            K.RandomRotation(degrees = 10, p = 0.3),
            K.RandomAffine(degrees = 10, p = 0.2)
        )

        self.jitter = ColorJitter(0.5, 0.5, 0.5, 0.5)

    @torch.no_grad()  # disable gradients for efficiency
    def forward(self, x: Tensor) -> Tensor:
        x_out = self.transforms(x)  # BxCxHxW
        if self._apply_color_jitter:
            x_out = self.jitter(x_out)
        return x_out
```


```{python}
class Preprocess(nn.Module):
    """Module to perform pre-process using Kornia on torch tensors."""

    @torch.no_grad()  # disable gradients for efficiency
    def forward(self, x) -> Tensor:
        x_tmp: np.ndarray = np.array(x)  # HxWxC
        x_out: Tensor = image_to_tensor(x_tmp, keepdim=True)  # CxHxW
        return x_out.float() / 255.0
```


```{python}
class EfficientLightB0(L.LightningModule):
    def __init__(self):
        super().__init__()
        # not the best model: expereiment yourself
        self.weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT
        self.model = torchvision.models.efficientnet_b0(weights = self.weights)
        self.batch_size = 64
        self.preprocess = self.weights.transforms()  # per sample transforms
        self.transform = DataAugmentation()  # per batch augmentation_kornia
        self.classes = 625
        self.train_accuracy = torchmetrics.Accuracy(task="multiclass", num_classes=self.classes)
        self.val_accuracy = torchmetrics.Accuracy(task="multiclass", num_classes=self.classes)
        #self.image_path = image_path

    def forward(self, x):
        return self.model(x)

    def compute_loss(self, y_hat, y):
        return F.cross_entropy(y_hat, y)

    def show_batch(self, win_size=(10, 10)):
        def _to_vis(data):
            return tensor_to_image(torchvision.utils.make_grid(data, nrow=8))

        # get a batch from the training set: try with `val_datlaoader` :)
        imgs, labels = next(iter(self.train_dataloader()))
        imgs_aug = self.transform(imgs)  # apply transforms
        # use matplotlib to visualize
        plt.figure(figsize=win_size)
        plt.imshow(_to_vis(imgs))
        plt.figure(figsize=win_size)
        plt.imshow(_to_vis(imgs_aug))

    def on_after_batch_transfer(self, batch, dataloader_idx):
        x, y = batch
        if self.trainer.training:
            x = self.transform(x)  # => we perform GPU/Batched data augmentation
        return x, y

    def training_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = self.compute_loss(y_hat, y)
        self.train_accuracy.update(y_hat, y)
        self.log("train_loss", loss, prog_bar=False)
        self.log("train_acc", self.train_accuracy, prog_bar=False)
        return loss

    def validation_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = self.compute_loss(y_hat, y)
        self.val_accuracy.update(y_hat, y)
        self.log("valid_loss", loss, prog_bar=False)
        self.log("valid_acc", self.val_accuracy, prog_bar=True)

    def configure_optimizers(self):
        optimizer = torch.optim.AdamW(self.model.parameters(), lr=1e-3)
        stepping_batches = self.trainer.estimated_stepping_batches
        scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-2, total_steps=stepping_batches)
        return [optimizer], [scheduler]

    def prepare_data(self):
        image_path = Path() / "deep_learning/data"
        train_dir = image_path / "train"
        valid_dir = image_path / "valid"
        test_dir = image_path / "test"
        
    def train_dataloader(self):
        image_path = Path() / "deep_learning/data"
        train_dir = image_path / "train"
        dataset = datasets.ImageFolder(root=train_dir, 
                                  transform=self.preprocess, # transform for the data
                                  target_transform=None)

        dataloader = DataLoader(dataset=dataset,
                                batch_size=self.batch_size, 
                                num_workers=1, 
                                shuffle=True)

        return dataloader

    def val_dataloader(self):
        image_path = Path() / "deep_learning/data"
        valid_dir = image_path / "valid"
        dataset = datasets.ImageFolder(root=valid_dir, 
                                 transform=self.preprocess, # transform for the data
                                 target_transform=None)

        dataloader = DataLoader(dataset=dataset, 
                                  batch_size=self.batch_size, 
                                  shuffle=False)

        dataset = CIFAR10(os.getcwd(), train=False, download=True, transform=self.preprocess)
        loader = DataLoader(dataset, batch_size=32)
        return dataloader

    def test_dataloader(self):
        image_path = Path() / "deep_learning/data"
        test_dir = image_path / "test"
        dataset = datasets.ImageFolder(root=test_dir, 
                                 transform=self.preprocess, # transform for the data
                                 target_transform=None)

        dataloader = DataLoader(dataset=test_data, 
                                batch_size=BATCH_SIZE, 
                                num_workers=1, 
                                shuffle=False)

        return dataloader

```


```{python}
model = EfficientLightB0()

trainer = L.Trainer(
    accelerator="auto",
    devices=1,
    max_epochs=10,
    logger=CSVLogger(save_dir="logs/"),
)

# Train the model âš¡
trainer.fit(model)
```